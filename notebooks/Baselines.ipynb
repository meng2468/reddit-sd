{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Method Notebook\n",
    "Model's for Stance Detection:\n",
    "- Random Baseline\n",
    "- Majority Baseline\n",
    "- Support Vector Machine with n-gram features\n",
    "\n",
    "Indivisual Datasets:\n",
    "- SemEval2016Task6\n",
    "- SethB\n",
    "- SethC\n",
    "- ARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from statistics import median\n",
    "\n",
    "import scipy\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SemEval2016T6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = '../data/SemEval2016Task6'\n",
    "os.listdir(file_dir)\n",
    "\n",
    "df_training = pd.read_csv(file_dir+'/trainingdata-all-annotations.txt', sep='\\t', encoding='latin1')\n",
    "df_test = pd.read_csv(file_dir + '/testdata-taskA-all-annotations.txt', sep='\\t', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(stance):\n",
    "    if stance == 'AGAINST':\n",
    "        return 0\n",
    "    if stance == 'FAVOR':\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "def get_ngrams(train_x, test_x):\n",
    "    ngram_words = CountVectorizer(analyzer='word', ngram_range=(1,3))\n",
    "    ngram_char = CountVectorizer(analyzer='char', ngram_range=(2,5))\n",
    "    \n",
    "    x_w = ngram_words.fit_transform(list(train_x))\n",
    "    x_c = ngram_char.fit_transform(list(train_x))\n",
    "    train_x = scipy.sparse.hstack([x_w, x_c])\n",
    "    \n",
    "    test_x_w = ngram_words.transform(list(test_x))\n",
    "    test_x_c = ngram_char.transform(list(test_x))\n",
    "    test_x = scipy.sparse.hstack([test_x_w,test_x_c])\n",
    "    \n",
    "    return train_x, test_x\n",
    "    \n",
    "def get_data_SemEval(target):\n",
    "    train = df_training[df_training.Target == target]\n",
    "    test = df_test[df_test.Target == target]\n",
    "    train_x = train['Tweet']\n",
    "    test_x = test['Tweet']\n",
    "    train_x, test_x = get_ngrams(train_x, test_x)\n",
    "    \n",
    "    train_y = train['Stance'].apply(map_labels).values\n",
    "    test_y = test['Stance'].apply(map_labels).values\n",
    "    return train_x, train_y, test_x, test_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def run_svm():\n",
    "    scores = []\n",
    "    accuracies = []\n",
    "    for target in df_training['Target'].unique():\n",
    "        train_x, train_y, test_x, test_y = get_data(target)\n",
    "        clf = svm.SVC(kernel='linear')\n",
    "        clf.fit(train_x, train_y)\n",
    "        preds = clf.predict(test_x)\n",
    "        score = f1_score(test_y, preds, average='macro')\n",
    "        correct = [1 if test_y[i] == preds[i] else 0  for i in range(len(test_y))]\n",
    "        acc = sum(correct)/len(correct)\n",
    "        print(correct) \n",
    "        print(target, ': ', score,'f1', acc,'acc')\n",
    "        scores.append(score)\n",
    "        accuracies.append(acc)\n",
    "    print('SemEval2016T6 avg:', sum(scores)/len(scores), sum(accuracies)/len(accuracies))\n",
    "    \n",
    "\n",
    "def majority_baseline():\n",
    "    scores = []\n",
    "    accuracies = []\n",
    "    for target in df_training['Target'].unique():\n",
    "        train_x, train_y, test_x, test_y = get_data(target)\n",
    "        majority = median(train_y)\n",
    "        preds = [majority for _ in test_y]\n",
    "        score = f1_score(test_y, preds, average='macro')\n",
    "        correct = [1 if test_y[i] == preds[i] else 0  for i in range(len(test_y))]\n",
    "        acc = sum(correct)/len(correct)\n",
    "        scores.append(score)\n",
    "        accuracies.append(acc)\n",
    "        print(target, ': ', score,'f1', acc,'acc')\n",
    "    print('SemEval2016T6 avg:', sum(scores)/len(scores), sum(accuracies)/len(accuracies))\n",
    "\n",
    "def random_baseline():\n",
    "    scores = []\n",
    "    accuracies = []\n",
    "    for target in df_training['Target'].unique():\n",
    "        train_x, train_y, test_x, test_y = get_data(target)\n",
    "        majority = median(train_y)\n",
    "        preds = [random.randint(0,2) for _ in test_y]\n",
    "        score = f1_score(test_y, preds, average='macro')\n",
    "        correct = [1 if test_y[i] == preds[i] else 0  for i in range(len(test_y))]\n",
    "        acc = sum(correct)/len(correct)\n",
    "        scores.append(score)\n",
    "        accuracies.append(acc)\n",
    "        print(target, ': ', score,'f1', acc,'acc')\n",
    "    print('SemEval2016T6 avg:', sum(scores)/len(scores), sum(accuracies)/len(accuracies))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atheism :  0.29208734411835524 f1 0.36818181818181817 acc\n",
      "Climate Change is a Real Concern :  0.2650551416273368 f1 0.31952662721893493 acc\n",
      "Feminist Movement :  0.2886752136752137 f1 0.3087719298245614 acc\n",
      "Hillary Clinton :  0.3195994264232841 f1 0.34915254237288135 acc\n",
      "Legalization of Abortion :  0.28956710753149767 f1 0.3357142857142857 acc\n",
      "SemEval2016T6 avg: 0.2909968466751375 0.3362694406624963\n"
     ]
    }
   ],
   "source": [
    "random_baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEthC and SethB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = '/Users/baconbaker/Documents/Studium/NLP/Project/reddit-sd/testing'\n",
    "os.listdir(file_dir)\n",
    "\n",
    "df = pd.read_csv(file_dir+'/SEthB.csv', sep=',', encoding='utf-8')\n",
    "df = pd.read_csv(file_dir+'/SEthC.csv', quotechar='`').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=.8)\n",
    "df_test = pd.concat([df, df_train]).drop_duplicates(keep=False)\n",
    "df_train = df_train[['text','label']]\n",
    "df_test = df_test[['text','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(stance):\n",
    "    if stance == 'against':\n",
    "        return 0\n",
    "    if stance == 'favor':\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "def get_ngrams(train_x, test_x):\n",
    "    ngram_words = CountVectorizer(analyzer='word', ngram_range=(1,3))\n",
    "    ngram_char = CountVectorizer(analyzer='char', ngram_range=(2,5))\n",
    "    \n",
    "    x_w = ngram_words.fit_transform(list(train_x))\n",
    "    x_c = ngram_char.fit_transform(list(train_x))\n",
    "    train_x = scipy.sparse.hstack([x_w, x_c])\n",
    "    \n",
    "    test_x_w = ngram_words.transform(list(test_x))\n",
    "    test_x_c = ngram_char.transform(list(test_x))\n",
    "    test_x = scipy.sparse.hstack([test_x_w,test_x_c])\n",
    "    \n",
    "    return train_x, test_x\n",
    "    \n",
    "def get_data_SEthB():\n",
    "    train_x = df_train['text']\n",
    "    test_x = df_test['text']\n",
    "    train_x, test_x = get_ngrams(train_x, test_x)\n",
    "    \n",
    "    train_y = df_train['label'].apply(map_labels).values\n",
    "    test_y = df_test['label'].apply(map_labels).values\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "def get_data_SEthC():\n",
    "    train_x = df_train['text']\n",
    "    test_x = df_test['text']\n",
    "    train_x, test_x = get_ngrams(train_x, test_x)\n",
    "    \n",
    "    train_y = df_train['label'].apply(map_labels).values\n",
    "    test_y = df_test['label'].apply(map_labels).values\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    " def run_svm():\n",
    "    scores = []\n",
    "    accuracies = []\n",
    "    train_x, train_y, test_x, test_y = get_data_SEthC()\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    clf.fit(train_x, train_y)\n",
    "    preds = clf.predict(test_x)\n",
    "    score = f1_score(test_y, preds, average='macro')\n",
    "    correct = [1 if test_y[i] == preds[i] else 0  for i in range(len(test_y))]\n",
    "    acc = sum(correct)/len(correct)\n",
    "    scores.append(score)\n",
    "    accuracies.append(acc)\n",
    "    print('SVM avg:', sum(scores)/len(scores), 'f1,',sum(accuracies)/len(accuracies), 'acc')\n",
    "    \n",
    "\n",
    "def majority_baseline():\n",
    "    scores = []\n",
    "    accuracies = []\n",
    "    train_x, train_y, test_x, test_y = get_data_SEthC()\n",
    "    majority = median(train_y)\n",
    "    preds = [majority for _ in test_y]\n",
    "    score = f1_score(test_y, preds, average='macro')\n",
    "    correct = [1 if test_y[i] == preds[i] else 0  for i in range(len(test_y))]\n",
    "    acc = sum(correct)/len(correct)\n",
    "    scores.append(score)\n",
    "    accuracies.append(acc)\n",
    "    print('Maj. Baseline avg:', sum(scores)/len(scores), 'f1,',sum(accuracies)/len(accuracies), 'acc')\n",
    "\n",
    "def random_baseline():\n",
    "    scores = []\n",
    "    accuracies = []\n",
    "    train_x, train_y, test_x, test_y = get_data_SEthC()\n",
    "    majority = median(train_y)\n",
    "    preds = [random.randint(0,2) for _ in test_y]\n",
    "    score = f1_score(test_y, preds, average='macro')\n",
    "    correct = [1 if test_y[i] == preds[i] else 0  for i in range(len(test_y))]\n",
    "    acc = sum(correct)/len(correct)\n",
    "    scores.append(score)\n",
    "    accuracies.append(acc)\n",
    "    print('Random Baseline avg:', sum(scores)/len(scores), 'f1,',sum(accuracies)/len(accuracies), 'acc')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEthC Results\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'run_svm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1fb365204a63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SEthC Results'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_svm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmajority_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrandom_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_svm' is not defined"
     ]
    }
   ],
   "source": [
    "print('SEthC Results')\n",
    "run_svm()\n",
    "majority_baseline()\n",
    "random_baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/Users/baconbaker/Documents/Studium/NLP/Project/reddit-sd/data/ARC'\n",
    "\n",
    "bodyfile = os.path.join(data_dir, \"arc_bodies.csv\")\n",
    "trainfile = os.path.join(data_dir, \"arc_stances_train.csv\")\n",
    "testfile = os.path.join(data_dir, \"arc_stances_test.csv\")\n",
    "\n",
    "# format data using pandas\n",
    "bodies = pd.read_csv(bodyfile)\n",
    "train_data = pd.read_csv(trainfile).merge(bodies, how='left', on='Body ID')\n",
    "test_data = pd.read_csv(testfile).merge(bodies, how='left', on='Body ID')\n",
    "useful_columns = [\"Headline\", \"Stance\", \"articleBody\"]\n",
    "renamed_columns = {'articleBody': \"text\", 'Stance': \"label\", 'Headline': \"target\"}\n",
    "train_data = train_data.loc[:, useful_columns].rename(columns=renamed_columns)\n",
    "test_data = test_data.loc[:, useful_columns].rename(columns=renamed_columns)\n",
    "\n",
    "df_training = train_data\n",
    "df_test = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(stance):\n",
    "    if stance == 'agree':\n",
    "        return 0\n",
    "    if stance == 'disagree':\n",
    "        return 1\n",
    "    if stance == 'unrelated':\n",
    "        return 2\n",
    "    return 3\n",
    "\n",
    "def get_ngrams(train_x, test_x):\n",
    "    ngram_words = CountVectorizer(analyzer='word', ngram_range=(1,3))\n",
    "    ngram_char = CountVectorizer(analyzer='char', ngram_range=(2,5))\n",
    "    \n",
    "    x_w = ngram_words.fit_transform(list(train_x))\n",
    "    x_c = ngram_char.fit_transform(list(train_x))\n",
    "    train_x = scipy.sparse.hstack([x_w, x_c])\n",
    "    \n",
    "    test_x_w = ngram_words.transform(list(test_x))\n",
    "    test_x_c = ngram_char.transform(list(test_x))\n",
    "    test_x = scipy.sparse.hstack([test_x_w,test_x_c])\n",
    "    \n",
    "    return train_x, test_x\n",
    "    \n",
    "def get_data_ARC(target):\n",
    "    train = df_training[df_training.target == target]\n",
    "    test = df_test[df_test.target == target]\n",
    "    train_x = train['text']\n",
    "    test_x = test['text']\n",
    "    train_x, test_x = get_ngrams(train_x, test_x)\n",
    "    \n",
    "    train_y = train['label'].apply(map_labels).values\n",
    "    test_y = test['label'].apply(map_labels).values\n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def majority_baseline():\n",
    "    print('Majority Baseline')\n",
    "    scores = []\n",
    "    accuracies = []\n",
    "    for target in tqdm(df_training['target'].unique()):\n",
    "        train_x, train_y, test_x, test_y = get_data_ARC(target)\n",
    "        majority = median(train_y)\n",
    "        preds = [majority for _ in test_y]\n",
    "        correct = [1 if test_y[i] == preds[i] else 0  for i in range(len(test_y))]\n",
    "        if len(correct) > 0:\n",
    "            acc = sum(correct)/len(correct)\n",
    "            score = f1_score(test_y, preds, average='macro')\n",
    "            scores.append(score)\n",
    "            accuracies.append(acc)\n",
    "        scores.append(score)\n",
    "        accuracies.append(acc)\n",
    "    print('Maj. Baseline avg:', sum(scores)/len(scores), sum(accuracies)/len(accuracies))\n",
    "\n",
    "def random_baseline():\n",
    "    print('Random Baseline')\n",
    "    scores = []\n",
    "    accuracies = []\n",
    "    for target in tqdm(df_training['target'].unique()):\n",
    "        train_x, train_y, test_x, test_y = get_data_ARC(target)\n",
    "        preds = [random.randint(0,3) for _ in test_y]\n",
    "        correct = [1 if test_y[i] == preds[i] else 0  for i in range(len(test_y))]\n",
    "        if len(correct) > 0:\n",
    "            acc = sum(correct)/len(correct)\n",
    "            score = f1_score(test_y, preds, average='macro')\n",
    "            scores.append(score)\n",
    "            accuracies.append(acc)\n",
    "    print('Random Baseline avg:', sum(scores)/len(scores), sum(accuracies)/len(accuracies))\n",
    "    \n",
    "def run_svm():\n",
    "    print('Running SVM')\n",
    "    scores = []\n",
    "    accuracies = []\n",
    "    for target in tqdm(df_training['target'].unique()):\n",
    "        train_x, train_y, test_x, test_y = get_data_ARC(target)\n",
    "        if len(test_y) < 2:\n",
    "            continue\n",
    "        clf = svm.SVC(kernel='linear')\n",
    "        if len(set(train_y)) == 1:\n",
    "            continue\n",
    "        clf.fit(train_x, train_y)\n",
    "        preds = clf.predict(test_x)\n",
    "        correct = [1 if test_y[i] == preds[i] else 0  for i in range(len(test_y))]\n",
    "        if len(correct) > 0:\n",
    "            acc = sum(correct)/len(correct)\n",
    "            score = f1_score(test_y, preds, average='macro')\n",
    "            scores.append(score)\n",
    "            accuracies.append(acc)\n",
    "        scores.append(score)\n",
    "        accuracies.append(acc)\n",
    "    print('SVM avg:', sum(scores)/len(scores), sum(accuracies)/len(accuracies))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/186 [00:00<00:21,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [01:02<00:00,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM avg: 0.4793858126028945 0.8058953478660416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
